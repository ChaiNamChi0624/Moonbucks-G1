{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t10WafCWgkWA",
        "outputId": "e1694352-2f31-4012-c82f-adf654cb3d1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\hozhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\hozhi\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1CRQlWpjC9K"
      },
      "source": [
        "**USA **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tqj4BVZi3Jc",
        "outputId": "ca36df85-06a7-4415-a5a4-88d289b1d422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "USA 1\n",
            "\n",
            "\n",
            "USA 2\n",
            "\n",
            "\n",
            "USA 3\n",
            "\n",
            "\n",
            "USA 4\n",
            "\n",
            "\n",
            "USA 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#pd.set_option('display.max_colwidth', 500)\n",
        "cnt = 1\n",
        "all_lists_USA = []\n",
        "for i in range(5):\n",
        "    df = pd.read_csv(r\"C:/Users/hozhi/OneDrive - 365.um.edu.my/UM/WIA2005_ADA/Group/data/USA{0}.txt\".format(cnt), delimiter='\\t', header=None)\n",
        "    df.columns = ['Paragraphs']\n",
        "    print(f\"\\nUSA {cnt}\\n\")\n",
        "    cnt += 1\n",
        "\n",
        "    def tokenize(text):\n",
        "        tokens = re.split(\"\\W+\", text)\n",
        "        return tokens\n",
        "    df['Tokenized message'] = df['Paragraphs'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    def removeStopwords(tokenized):\n",
        "        filtered = [i for i in tokenized if i not in stop_words]\n",
        "        return filtered\n",
        "\n",
        "    df['Filtered'] = df['Tokenized message'].apply(lambda x: removeStopwords(x))\n",
        "    # print(df)\n",
        "\n",
        "    for j in range(len(df.index)):\n",
        "      all_words = [paragraphs for paragraphs in df['Filtered']]\n",
        "      all_lists_USA += all_words[j]\n",
        "# print(all_lists_USA)\n",
        "# print(len(all_lists_USA))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-b_HtqklGUr"
      },
      "source": [
        "**Thailand**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kyg-YculevP",
        "outputId": "9b65868a-54fb-40d8-e8d9-af5b13da8f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Thailand 1\n",
            "\n",
            "\n",
            "Thailand 2\n",
            "\n",
            "\n",
            "Thailand 3\n",
            "\n",
            "\n",
            "Thailand 4\n",
            "\n",
            "\n",
            "Thailand 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cnt = 1\n",
        "all_lists_TH = []\n",
        "for i in range(5):\n",
        "    df = pd.read_csv(r\"C:/Users/hozhi/OneDrive - 365.um.edu.my/UM/WIA2005_ADA/Group/data/Thailand {0}.txt\".format(cnt), delimiter='\\t', header=None)\n",
        "    df.columns = ['Paragraphs']\n",
        "    print(f\"\\nThailand {cnt}\\n\")\n",
        "    cnt += 1\n",
        "\n",
        "    def tokenize(text):\n",
        "        tokens = re.split(\"\\W+\", text)\n",
        "        return tokens\n",
        "    df['Tokenized message'] = df['Paragraphs'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    def removeStopwords(tokenized):\n",
        "        filtered = [i for i in tokenized if i not in stop_words]\n",
        "        return filtered\n",
        "\n",
        "    df['Filtered'] = df['Tokenized message'].apply(lambda x: removeStopwords(x))\n",
        "    # print(df)\n",
        "\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "        all_words = [paragraphs for paragraphs in df['Filtered']]\n",
        "        all_lists_TH += all_words[i]\n",
        "    # print(len(all_lists_TH))\n",
        "\n",
        "# print(all_lists_TH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeAeHoqjlyCC"
      },
      "source": [
        "**New Zealand**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9_UtLwMl2tF",
        "outputId": "4efbd4cd-081d-44b3-bc12-5513d6eefa8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New Zealand 1\n",
            "\n",
            "\n",
            "New Zealand 2\n",
            "\n",
            "\n",
            "New Zealand 3\n",
            "\n",
            "\n",
            "New Zealand 4\n",
            "\n",
            "\n",
            "New Zealand 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cnt = 1\n",
        "all_lists_NZ = []\n",
        "for i in range(5):\n",
        "    df = pd.read_csv(r\"C:/Users/hozhi/OneDrive - 365.um.edu.my/UM/WIA2005_ADA/Group/data/NZ {0}.txt\".format(cnt), delimiter='\\t', header=None)\n",
        "    df.columns = ['Paragraphs']\n",
        "    print(f\"\\nNew Zealand {cnt}\\n\")\n",
        "    cnt += 1\n",
        "\n",
        "    def tokenize(text):\n",
        "        tokens = re.split(\"\\W+\", text)\n",
        "        return tokens\n",
        "    df['Tokenized message'] = df['Paragraphs'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    def removeStopwords(tokenized):\n",
        "        filtered = [i for i in tokenized if i not in stop_words]\n",
        "        return filtered\n",
        "\n",
        "    df['Filtered'] = df['Tokenized message'].apply(lambda x: removeStopwords(x))\n",
        "    # print(df)\n",
        "\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "        all_words = [paragraphs for paragraphs in df['Filtered']]\n",
        "        all_lists_NZ += all_words[i]\n",
        "    # print(len(all_lists_NZ))\n",
        "\n",
        "# print(all_lists_NZ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5r2epocmKnY"
      },
      "source": [
        "**Hong Kong**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoolAlipmO1e",
        "outputId": "5c795c32-e605-4878-e39b-ce0f446538f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Hong Kong 1\n",
            "\n",
            "\n",
            " Hong Kong 2\n",
            "\n",
            "\n",
            " Hong Kong 3\n",
            "\n",
            "\n",
            " Hong Kong 4\n",
            "\n",
            "\n",
            " Hong Kong 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cnt = 1\n",
        "all_lists_HK = []\n",
        "for i in range(5):\n",
        "    df = pd.read_csv(r\"C:/Users/hozhi/OneDrive - 365.um.edu.my/UM/WIA2005_ADA/Group/data/HK {0}.txt\".format(cnt), delimiter='\\t', header=None)\n",
        "    df.columns = ['Paragraphs']\n",
        "    print(f\"\\n Hong Kong {cnt}\\n\")\n",
        "    cnt += 1\n",
        "\n",
        "    def tokenize(text):\n",
        "        tokens = re.split(\"\\W+\", text)\n",
        "        return tokens\n",
        "    df['Tokenized message'] = df['Paragraphs'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    def removeStopwords(tokenized):\n",
        "        filtered = [i for i in tokenized if i not in stop_words]\n",
        "        return filtered\n",
        "\n",
        "    df['Filtered'] = df['Tokenized message'].apply(lambda x: removeStopwords(x))\n",
        "    # print(df)\n",
        "\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "        all_words = [paragraphs for paragraphs in df['Filtered']]\n",
        "        all_lists_HK += all_words[i]\n",
        "    # print(len(all_lists_HK))\n",
        "\n",
        "# print(all_lists_HK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCr9-nAumpa0"
      },
      "source": [
        "**Dubai**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjQZaziZmr_B",
        "outputId": "bd2698f2-51b5-45c7-e1b6-9355e2b66af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dubai 1\n",
            "\n",
            "409\n",
            "\n",
            "Dubai 2\n",
            "\n",
            "580\n",
            "\n",
            "Dubai 3\n",
            "\n",
            "741\n",
            "\n",
            "Dubai 4\n",
            "\n",
            "1242\n",
            "\n",
            "Dubai 5\n",
            "\n",
            "1461\n"
          ]
        }
      ],
      "source": [
        "cnt = 1\n",
        "all_lists_DB = []\n",
        "for i in range(5):\n",
        "    df = pd.read_csv(r\"C:/Users/hozhi/OneDrive - 365.um.edu.my/UM/WIA2005_ADA/Group/data/DB {0}.txt\".format(cnt), delimiter='\\t', header=None)\n",
        "    df.columns = ['Paragraphs']\n",
        "    print(f\"\\nDubai {cnt}\\n\")\n",
        "    cnt += 1\n",
        "\n",
        "    def tokenize(text):\n",
        "        tokens = re.split(\"\\W+\", text)\n",
        "        return tokens\n",
        "    df['Tokenized message'] = df['Paragraphs'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "    def removeStopwords(tokenized):\n",
        "        filtered = [i for i in tokenized if i not in stop_words]\n",
        "        return filtered\n",
        "\n",
        "    df['Filtered'] = df['Tokenized message'].apply(lambda x: removeStopwords(x))\n",
        "    # print(df)\n",
        "\n",
        "\n",
        "    for i in range(len(df.index)):\n",
        "        all_words = [paragraphs for paragraphs in df['Filtered']]\n",
        "        all_lists_DB += all_words[i]\n",
        "    print(len(all_lists_DB))\n",
        "\n",
        "# print(all_lists_DB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqdgqnoQTQ9I"
      },
      "source": [
        "**Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vader Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        " \n",
        "# function to print sentiments\n",
        "# of the sentence.\n",
        "def sentiment_scores(sentence, country):\n",
        " \n",
        "    # Create a SentimentIntensityAnalyzer object.\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        " \n",
        "    # polarity_scores method of SentimentIntensityAnalyzer\n",
        "    # object gives a sentiment dictionary.\n",
        "    # which contains pos, neg, neu, and compound scores.\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "\n",
        "    print(f'\\nCountry: {country}') \n",
        "    print(\"Overall sentiment dictionary is: \", sentiment_dict)\n",
        "    print(\"Articles are rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
        "    print(\"Articles are rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "    print(\"Articles are rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
        " \n",
        "    print(\"Economic Situation Overall is Rated As\", end = \" \")\n",
        " \n",
        "    # decide sentiment as positive, negative and neutral\n",
        "    if sentiment_dict['compound'] >= 0.05 :\n",
        "        print(\"Positive\")\n",
        " \n",
        "    elif sentiment_dict['compound'] <= - 0.05 :\n",
        "        print(\"Negative\")\n",
        " \n",
        "    else :\n",
        "        print(\"Neutral\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Country: United States of America\n",
            "Overall sentiment dictionary is:  {'neg': 0.145, 'neu': 0.774, 'pos': 0.081, 'compound': -0.9996}\n",
            "Articles are rated as  14.499999999999998 % Negative\n",
            "Articles are rated as  77.4 % Neutral\n",
            "Articles are rated as  8.1 % Positive\n",
            "Economic Situation Overall is Rated As Negative\n",
            "\n",
            "Country: Thailand\n",
            "Overall sentiment dictionary is:  {'neg': 0.072, 'neu': 0.759, 'pos': 0.169, 'compound': 0.9997}\n",
            "Articles are rated as  7.199999999999999 % Negative\n",
            "Articles are rated as  75.9 % Neutral\n",
            "Articles are rated as  16.900000000000002 % Positive\n",
            "Economic Situation Overall is Rated As Positive\n",
            "\n",
            "Country: New Zealand\n",
            "Overall sentiment dictionary is:  {'neg': 0.139, 'neu': 0.754, 'pos': 0.108, 'compound': -0.9975}\n",
            "Articles are rated as  13.900000000000002 % Negative\n",
            "Articles are rated as  75.4 % Neutral\n",
            "Articles are rated as  10.8 % Positive\n",
            "Economic Situation Overall is Rated As Negative\n",
            "\n",
            "Country: Hong Kong\n",
            "Overall sentiment dictionary is:  {'neg': 0.11, 'neu': 0.806, 'pos': 0.084, 'compound': -0.9922}\n",
            "Articles are rated as  11.0 % Negative\n",
            "Articles are rated as  80.60000000000001 % Neutral\n",
            "Articles are rated as  8.4 % Positive\n",
            "Economic Situation Overall is Rated As Negative\n",
            "\n",
            "Country: Dubai\n",
            "Overall sentiment dictionary is:  {'neg': 0.009, 'neu': 0.764, 'pos': 0.227, 'compound': 0.9998}\n",
            "Articles are rated as  0.8999999999999999 % Negative\n",
            "Articles are rated as  76.4 % Neutral\n",
            "Articles are rated as  22.7 % Positive\n",
            "Economic Situation Overall is Rated As Positive\n"
          ]
        }
      ],
      "source": [
        "sentiment_scores(' '.join([str(x) for x in all_lists_USA]), 'United States of America')\n",
        "sentiment_scores(' '.join([str(x) for x in all_lists_TH]), 'Thailand')\n",
        "sentiment_scores(' '.join([str(x) for x in all_lists_NZ]), 'New Zealand')\n",
        "sentiment_scores(' '.join([str(x) for x in all_lists_HK]), 'Hong Kong')\n",
        "sentiment_scores(' '.join([str(x) for x in all_lists_DB]), 'Dubai')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvVLrlm6GYOS"
      },
      "source": [
        "Plotting Graph"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Algo Pblm 1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "efe6b1d779ca055e3793b7e365c74238dac1730aaa9dd81c00478ab0323312de"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
